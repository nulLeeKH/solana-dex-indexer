#!/usr/bin/env npx ts-node

import { Connection, PublicKey } from '@solana/web3.js';
import { AppDataSource } from '../src/config/database';
import { Pool } from '../src/models/Pool';
import { PoolQueue, QueueStatus, PoolType } from '../src/models/PoolQueue';
import { logger } from '../src/utils/logger';
import { rpcProvider } from '../src/utils/rpcProvider';
import { rpcRateLimiter } from '../src/utils/rpcRateLimiter';
import { withExponentialBackoff } from '../src/utils/exponentialBackoff';
import * as dotenv from 'dotenv';

dotenv.config();

/**
 * Sync: ì „ì²´ í’€ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì™€ì„œ queueì— í•œë²ˆì— ì‚½ì…í•¨
 * íŒŒë¼ë¯¸í„° ì„¤ì •ì— ë”°ë¼ í˜„ì¬ poolsì— ì¸ë±ì‹±ë˜ì§€ ì•Šì€ê²ƒë§Œ ë„£ë„ë¡ ì„¤ì •í•  ìˆ˜ ìˆìŒ
 */
class PoolSync {
  private connection: Connection;
  private poolRepository = AppDataSource.getRepository(Pool);
  private poolQueueRepository = AppDataSource.getRepository(PoolQueue);
  
  private readonly programId = process.env['METEORA_PROGRAM_ID'] || 'cpamdpZCGKUy5JxQXB4dcpGPiikHawvSWAd6mEn1sGG';
  private readonly batchSize = parseInt(process.env['SYNC_BATCH_SIZE'] || '1000');
  private readonly onlyMissing = process.env['SYNC_ONLY_MISSING'] === 'true';
  private readonly skipCompleted = process.env['SYNC_SKIP_COMPLETED'] === 'true';
  
  private discoveredCount = 0;
  private queuedCount = 0;
  private skippedCount = 0;

  constructor() {
    this.connection = rpcProvider.getConnection();
  }

  async performFullSync(): Promise<void> {
    try {
      logger.info('ğŸ“‹ Starting full pool discovery and sync...');
      
      if (!AppDataSource.isInitialized) {
        await AppDataSource.initialize();
        logger.info('âœ… Database initialized');
      }

      logger.info(`ğŸ“‹ Program: ${this.programId}`);
      logger.info(`ğŸ“‹ Batch size: ${this.batchSize}`);
      logger.info(`ğŸ“‹ Only missing pools: ${this.onlyMissing}`);
      logger.info(`ğŸš« Skip completed pools: ${this.skipCompleted}`);

      const startTime = Date.now();
      
      // Get all pool accounts from the program
      await this.discoverAllPools();
      
      const duration = Date.now() - startTime;
      logger.info(`âœ… Sync completed in ${Math.round(duration / 1000)}s`);
      logger.info(`ğŸ“Š Results: ${this.discoveredCount} discovered, ${this.queuedCount} queued, ${this.skippedCount} skipped`);
      
    } catch (error) {
      logger.error('âŒ Sync failed:', error);
      throw error;
    }
  }

  private async discoverAllPools(): Promise<void> {
    try {
      logger.info('ğŸ” Discovering all DAMMv2 pool accounts...');

      // Get all program accounts with the correct data size
      const accounts = await rpcRateLimiter.execute(
        () => withExponentialBackoff(
          () => this.connection.getProgramAccounts(new PublicKey(this.programId), {
            filters: [
              { dataSize: 1112 } // DAMMv2 pool data size
            ]
          }),
          'getProgramAccounts'
        ),
        'getProgramAccounts'
      );

      this.discoveredCount = accounts.length;
      logger.info(`ğŸ” Discovered ${this.discoveredCount} pool accounts`);

      if (this.discoveredCount === 0) {
        logger.warn('âš ï¸ No pool accounts found');
        return;
      }

      // Process in batches to avoid overwhelming the database
      const poolAddresses = accounts.map(account => account.pubkey.toString());
      await this.processBatches(poolAddresses);

    } catch (error) {
      logger.error('âŒ Failed to discover pools:', error);
      throw error;
    }
  }

  private async processBatches(poolAddresses: string[]): Promise<void> {
    const totalBatches = Math.ceil(poolAddresses.length / this.batchSize);
    
    for (let i = 0; i < totalBatches; i++) {
      const start = i * this.batchSize;
      const end = Math.min(start + this.batchSize, poolAddresses.length);
      const batch = poolAddresses.slice(start, end);
      
      logger.info(`ğŸ“‹ Processing batch ${i + 1}/${totalBatches} (${batch.length} pools)`);
      
      await this.processBatch(batch);
      
      // Small delay between batches to avoid overwhelming the database
      if (i < totalBatches - 1) {
        await new Promise(resolve => setTimeout(resolve, 100));
      }
    }
  }

  private async processBatch(addresses: string[]): Promise<void> {
    let batchQueued = 0;
    let batchSkipped = 0;

    for (const address of addresses) {
      const result = await this.addToQueue(address);
      if (result) {
        batchQueued++;
      } else {
        batchSkipped++;
      }
    }

    this.queuedCount += batchQueued;
    this.skippedCount += batchSkipped;
    
    logger.info(`ğŸ“‹ Batch complete: ${batchQueued} queued, ${batchSkipped} skipped`);
  }

  private async addToQueue(poolAddress: string): Promise<boolean> {
    try {
      // Check if we should skip existing pools
      if (this.onlyMissing) {
        // Check if pool already exists in the pools table
        const existingPool = await this.poolRepository.findOne({
          where: { address: poolAddress }
        });
        
        if (existingPool) {
          logger.debug(`ğŸ“‹ Pool already indexed, skipping: ${poolAddress}`);
          return false;
        }
      }

      // Check existing queue item
      const existingQueueItem = await this.poolQueueRepository.findOne({
        where: { address: poolAddress }
      });

      if (existingQueueItem) {
        // PROCESSING ìƒíƒœë©´ ìŠ¤í‚µ (ë‹¤ë¥¸ consumerê°€ ì²˜ë¦¬ì¤‘)
        if (existingQueueItem.status === QueueStatus.PROCESSING) {
          logger.debug(`ğŸ“‹ Already processing, skipping: ${poolAddress}`);
          return false;
        }
        
        // COMPLETED ìƒíƒœì¼ ë•Œ ìŠ¤í‚µ ì˜µì…˜ í™•ì¸
        if (existingQueueItem.status === QueueStatus.COMPLETED && this.skipCompleted) {
          logger.debug(`ğŸ“‹ Skipping completed pool (SYNC_SKIP_COMPLETED=true): ${poolAddress}`);
          return false;
        }
        
        // PENDING/FAILED/COMPLETED ìƒíƒœ ì²˜ë¦¬
        if (existingQueueItem.status === QueueStatus.PENDING || 
            existingQueueItem.status === QueueStatus.FAILED ||
            existingQueueItem.status === QueueStatus.COMPLETED) {
          
          const updateData: any = {
            status: QueueStatus.PENDING, // ë‹¤ì‹œ ì²˜ë¦¬í•˜ë„ë¡ PENDINGìœ¼ë¡œ ë³€ê²½
            retryCount: 0
          };
          
          // COMPLETED/FAILED ìƒíƒœì¼ ë•ŒëŠ” ê¸°ë³¸ ìš°ì„ ìˆœìœ„(2)ë¡œ ë¦¬ì…‹
          // PENDING ìƒíƒœì¼ ë•Œë§Œ ì¡°ê±´ë¶€ ìŠ¹ê²©
          if (existingQueueItem.status === QueueStatus.COMPLETED || existingQueueItem.status === QueueStatus.FAILED) {
            updateData.priority = 2; // Syncì˜ ê¸°ë³¸ ìš°ì„ ìˆœìœ„
            logger.debug(`ğŸ“‹ Reset to priority 2 for SYNC: ${poolAddress}`);
          } else if (existingQueueItem.status === QueueStatus.PENDING && existingQueueItem.priority === 3) {
            updateData.priority = 2;
            logger.debug(`ğŸ“‹ Promoted priority 3 â†’ 2 for SYNC: ${poolAddress}`);
          } else {
            logger.debug(`ğŸ“‹ Maintained priority ${existingQueueItem.priority} for SYNC: ${poolAddress}`);
          }
          
          await this.poolQueueRepository.update({ address: poolAddress }, updateData);
          return false; // Not newly added
        }
      }

      // Add new item to queue
      await this.poolQueueRepository.save({
        address: poolAddress,
        status: QueueStatus.PENDING,
        type: PoolType.METEORA_DAMMV2,
        priority: 2, // Medium priority
        retryCount: 0,
      });
      
      logger.debug(`ğŸ“‹ Queued from SYNC: ${poolAddress}`);
      return true;
    } catch (error: any) {
      if (error.code === '23505') {
        // ì¤‘ë³µ í‚¤ ì—ëŸ¬ - ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ ì´ë¯¸ ì¶”ê°€í–ˆìŒ
        logger.debug(`ğŸ“‹ Pool ${poolAddress} already queued by another process`);
        return false;
      }
      logger.error(`Failed to add ${poolAddress} to queue:`, error);
      return false;
    }
  }

  getStatus(): {
    discoveredCount: number;
    queuedCount: number;
    skippedCount: number;
    programId: string;
    batchSize: number;
    onlyMissing: boolean;
  } {
    return {
      discoveredCount: this.discoveredCount,
      queuedCount: this.queuedCount,
      skippedCount: this.skippedCount,
      programId: this.programId,
      batchSize: this.batchSize,
      onlyMissing: this.onlyMissing,
    };
  }
}

async function runSync() {
  try {
    logger.info('ğŸš€ Starting DAMMv2 Pool Sync Process...');
    
    const sync = new PoolSync();
    
    const startTime = Date.now();
    await sync.performFullSync();
    const duration = Date.now() - startTime;
    
    const status = sync.getStatus();
    logger.info(`âœ… Sync completed in ${Math.round(duration / 1000)}s`);
    logger.info(`ğŸ“Š Final results: ${status.discoveredCount} discovered, ${status.queuedCount} queued, ${status.skippedCount} skipped`);
    
    // Close database connection
    if (AppDataSource.isInitialized) {
      await AppDataSource.destroy();
      logger.info('âœ… Database connection closed');
    }
    
    process.exit(0);
    
  } catch (error) {
    logger.error('âŒ Sync failed:', error);
    
    if (AppDataSource.isInitialized) {
      await AppDataSource.destroy();
    }
    
    process.exit(1);
  }
}

runSync().catch(async (error) => {
  logger.error('âŒ Unhandled error in sync:', error);
  if (AppDataSource.isInitialized) {
    await AppDataSource.destroy();
  }
  process.exit(1);
});

// Handle graceful shutdown
process.on('SIGINT', async () => {
  logger.info('ğŸ›‘ Sync interrupted by user');
  if (AppDataSource.isInitialized) {
    await AppDataSource.destroy();
  }
  process.exit(0);
});

process.on('SIGTERM', async () => {
  logger.info('ğŸ›‘ Sync terminated');
  if (AppDataSource.isInitialized) {
    await AppDataSource.destroy();
  }
  process.exit(0);
});